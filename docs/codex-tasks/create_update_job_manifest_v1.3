TARGET_SCRIPT: <SET THIS ONCE — example: jobs/vendor_input_processing/mapping_method_training/glue_script.py>

DERIVED (DO NOT EDIT — Codex must compute these from TARGET_SCRIPT)
- JOB_DIR = directory containing TARGET_SCRIPT
- MANIFEST_PATH = JOB_DIR/job_manifest.yaml
- ENTRYPOINT = basename(TARGET_SCRIPT)   # e.g., glue_script.py
- JOB_ID = basename(JOB_DIR)             # parent folder name of TARGET_SCRIPT

TASK TYPE: Generate/Refresh job_manifest.yaml (Automation-enabling, low-TBD, evidence-based) — v1.3

GOAL
Create or update the manifest located next to TARGET_SCRIPT:
- MANIFEST_PATH

PHASE RULE
Phase 1 documentation only: describe WHAT EXISTS. No refactoring. No behavior changes.

ALLOWED SOURCES (ONLY)
1) TARGET_SCRIPT (including helper functions defined in the same file)

HARD CONSTRAINTS
- Do NOT modify TARGET_SCRIPT.
- Do NOT modify any other file.
- Do NOT guess values. If a value cannot be supported from TARGET_SCRIPT, use "TBD".
- BUT: If a value IS provable from TARGET_SCRIPT (including by “implicit failure”), you MUST populate it (TBD is not allowed in that case).

MANIFEST SCHEMA (MUST MATCH EXACTLY; keep this key order and ALWAYS include ALL sections)
job_id: JOB_ID
glue_job_name: TBD
runtime: TBD
entrypoint: ENTRYPOINT

parameters:
  - TBD

inputs: []          # MUST be a list; may be empty only if script reads no external artifacts
outputs: []         # MUST be a list; may be empty only if script writes no external artifacts
config_files: []    # MUST be a list; may be empty if script uses no static configs

side_effects:
  deletes_inputs: TBD
  overwrites_outputs: TBD

logging_and_receipt:
  writes_run_receipt: TBD
  run_receipt_bucket: TBD
  run_receipt_key_pattern: TBD
  counters_observed: []

notes:
  - "Phase 1 reverse-documentation: derived only from TARGET_SCRIPT; unknowns are marked TBD."
  - "TBD_EXPLANATIONS: (auto-filled list below)"

LIST EXPANSION RULE (CRITICAL FOR AUTOMATION)
- inputs, outputs, config_files MUST enumerate ALL discovered artifacts.
- Never keep a single placeholder entry. Expand lists with one entry per artifact.
- If none exist for a list, keep it as an empty list [].

EXTRACTION REQUIREMENTS (CORE)

A) Identify runtime parameters (MUST)
- Extract the exact parameter list from explicit parsing in code (e.g., getResolvedOptions(...) list, argparse definitions, sys.argv parsing).
- Populate `parameters` using exact casing and order as in code.
- If parameter parsing is not explicit in code, leave parameters as TBD AND explain why in TBD_EXPLANATIONS.

B) Determine runtime (MUST NOT BE TBD IF DECIDABLE)
- If code instantiates/uses GlueContext/SparkContext/SparkSession OR uses spark.read / DataFrames / Glue dynamic frames => runtime=pyspark.
- Else if it is plain Python (no Spark/GlueContext usage) => runtime=python_shell.
- Else runtime=TBD and explain.

C) Enumerate ALL FINAL OUTPUT ARTIFACTS (MUST)
You MUST enumerate every distinct “final output artifact” that can persist beyond the job:
- Any S3 put_object/copy_object/upload_file (directly or via helper) that writes a named key
- Any Spark write/save to an s3:// path that is not deleted again by the same job

IMPORTANT EXCLUSION:
- Do NOT list job-internal temp/staging prefixes as outputs if the script deletes them in the same run.

For each final output artifact, add one outputs[] entry with:
- bucket: use ${OUTPUT_BUCKET}/${INPUT_BUCKET} if derived from those params; else literal if hardcoded; else TBD
- key_pattern: represent the final key/prefix using ${...} placeholders only for parameters that truly exist in `parameters`
- format: set only if code makes it unambiguous; else TBD
- required: decide via rule E (must not be TBD if decidable)

D) Enumerate ALL EXTERNAL INPUT ARTIFACTS THE JOB READS (MUST)
You MUST enumerate input reads including (directly or via helpers):
- boto3 get_object / head_object / download_file / list_objects used to locate a file that is then read
- spark.read.* from S3 paths
- Any other explicit S3 reads

For each external read artifact, classify it as either:
- config_files[] (static config/control), OR
- inputs[] (data / reference / snapshot / pipeline artifact)

CLASSIFICATION RULE (THIS FIXES THE “REFERENCE MISFILED AS CONFIG” PROBLEM)
1) Put into config_files[] ONLY if the S3 key is clearly a STATIC CONFIG file, i.e. ANY of:
   - key contains "/configuration-files/" OR "/configs/" OR "/config/" (path-based)
   - filename contains "_config_" OR ends with "_config.json" OR contains "denylist" (name-based)
   - the script treats it as operator config (loaded once to control parsing/mapping behavior) AND the key is NOT run/version-scoped (no run_id/timestamp pattern)

2) Everything else MUST be listed under inputs[] (not config_files[]), including:
   - mapping references, rules snapshots, stable training sets, evidence sets
   - any file under canonical_mappings/, stable_training_sets/, mappingMethodTraining/, or other pipeline artifact folders
   - any file selected as “latest matching wildcard” (e.g., Category_Mapping_Reference_*.json)
   - any file whose naming indicates versioning (timestamp/run_id in filename)

IMPORTANT: Do NOT classify something as config_files merely because a constant/variable contains words like REFERENCE or SNAPSHOT.
Only use the path/name rules above.

For each inputs[] entry:
- bucket: use ${INPUT_BUCKET}/${OUTPUT_BUCKET} if derived from those params; else literal if hardcoded; else TBD
- key_pattern: represent prefix/wildcard search patterns when listing is used (e.g., `${prefix}/Category_Mapping_Reference_*`)
- format: set only if code makes it unambiguous; else TBD
- required: decide via rule E (must not be TBD if decidable)

For each config_files[] entry:
- s3_bucket: use ${INPUT_BUCKET}/${OUTPUT_BUCKET} if derived; else literal; else TBD
- s3_key_pattern: key/pattern
- required: decide via rule E (for configs, treat missing as failure unless explicitly handled)
- repo_path: always TBD unless script explicitly references repo paths (rare)

E) REQUIREDNESS DECISION RULE (AUTOMATION-CRITICAL)
For each input/output/config_file, set required=true/false using BOTH explicit handling AND implicit failure.

Inputs/config_files — set required=false ONLY if you can prove a successful path exists when the artifact is missing.
Inputs/config_files — set required=true if missing would cause job failure and there is no successful “missing handled” path.
Evidence for required=true includes ANY of:
- unguarded boto3 get_object/head_object/download_file where exceptions are not caught to allow a successful continuation
- unguarded spark.read.* without a guard that allows success without it
- artifact is used before any “missing -> write empty output -> return success” branch

IMPORTANT:
- Even if the script has no explicit “if missing then fail”, an unguarded read that would throw and terminate the job is sufficient evidence to set required=true.
- Do NOT leave required as TBD in that case.

Outputs — required=true if written on ALL successful exit paths.
Outputs — required=false if there exists any successful exit path where job completes without writing it.
Outputs — required=TBD only if code does not allow proving either.

F) Side effects booleans (MUST NOT BE TBD IF DECIDABLE)
- deletes_inputs:
  - true only if the script deletes objects that belong to any declared inputs[] bucket/prefix
  - false if deletes happen only in temp output areas or no deletes occur
  - TBD only if delete targets cannot be determined from code

- overwrites_outputs:
  - true if the script writes/copies/puts to deterministic final keys reused across runs OR explicitly overwrites
  - false if final output keys are always run-unique and no overwrite occurs
  - TBD only if not provable

G) writes_run_receipt (MUST BE true OR false; never TBD)
- true only if the script writes a dedicated run-receipt artifact to S3
- false if no such writing exists
- If true: fill run_receipt_bucket/key_pattern if determinable; otherwise set them TBD with explanation.

H) counters_observed
- If the script clearly logs/returns named counters in a structured way (e.g., a counters dict written to S3), list them.
- Otherwise leave counters_observed as an empty list [] (not TBD).

TBD_EXPLANATIONS REQUIREMENT (MUST)
Append bullet list under notes where each item is:
- "<field_path>: TBD — <why not provable from TARGET_SCRIPT>"

QUALITY GATES (MUST PASS)
1) Manifest MUST include ALL top-level sections exactly as in MANIFEST SCHEMA (no omissions).
2) inputs/outputs/config_files MUST be fully enumerated lists (no single placeholder entry).
3) Every mandatory external read MUST have required=true (no TBD for that case).
4) Any STATIC CONFIG file must be in config_files[]; any reference/snapshot/training artifact must be in inputs[].
5) Every final external write must be represented in outputs[] (excluding self-deleted temp prefixes).
6) writes_run_receipt must be true or false (never TBD).
7) Side-effect booleans must not be TBD when decidable.
8) Remaining TBDs must appear in TBD_EXPLANATIONS.

PR REQUIREMENTS
- Create a PR that changes ONLY: MANIFEST_PATH
- PR description MUST include:
  1) Evidence Map: each non-TBD manifest field -> concrete evidence pointer in code (function/variable/literal key fragment/control-flow)
  2) Confirmation that all remaining TBDs appear in TBD_EXPLANATIONS
  3) Confirmation that TARGET_SCRIPT was not modified
